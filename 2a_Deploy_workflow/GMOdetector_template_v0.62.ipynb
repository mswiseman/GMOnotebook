{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size = 6>*GMOdetector notebook*: Template to analyze a new batch of images</font><br>\n",
    "**Notebook template for applying routine hyperspectral/segmentation cross-analysis phenomics workflow over new datasets** (v.0.6.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div>\n",
    "<img src=\"Figures/WorkflowFlowchart.png\" width = 500>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workflow, images taken with the macroPhor Array dual RGB/hyperspectral imaging platform are analyzed by a workflow in which regression quantifies fluorescent signals in hyperspectral images, deep learning segments RGB images into different tissues, and these datasets are cross-referenced to produce statistics on growth of transgenic callus and shoot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment ID and description:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "Provide a short description of the experiment in the below box. This should include unique identifier codes for the experiment, along with a short description of genotypes and treatments studied. The timepoint should also be included. </div>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dataset (Phase ID and week timepoint): \"ENTER_NOTE\"\n",
    "\n",
    "This notebook comes from a template which we use in R scripts to automatically generate a notebook for each \n",
    "transformation GWAS phase x timepoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true"
   },
   "source": [
    "# Parameters for analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "The below variables must be modified appropriately every time this workflow is run over new images.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data location\n",
    "The `data` variable below provides the **complete** path to the folder containing data to be analyzed. This should include all folders and subfolders in which the data of interest is organized by. For the organizational system used for our lab's data, this should follow the format \"/Experiment/Subexperiment/Timepoint/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=\"ENTER_DATA_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample information\n",
    "Every experiment has a spreadsheet of metadata to organize treatment and genotype information for each plate, prepare labels, and randomize plates. [For details, see tutorial on preparing this spreadsheet](https://github.com/naglemi/GMOnotebook/blob/master/1_Decide_parameters/1_Metadata_and_randomization/1-Generate_randomization_scheme.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomization_datasheet=\"ENTER_RANDOMIZATION_DATASHEET_PATH\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection of missing or contaminated explants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the `missing_explants` variable to `\"Automatic\"` or to the path of manually prepared data file. [For details, see this tutorial and example file](https://github.com/naglemi/GMOnotebook/tree/master/1_Decide_parameters/3_Other_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_explants=\"Automatic\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fluorescent protein settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing weights for fluorescent proteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[See this notebook for details on all below fluorescent protein settings.](https://github.com/naglemi/GMOnotebook/blob/master/1_Decide_parameters/3_Other_parameters/3_Hyperspectral_settings.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL known fluorescent components in the sample should be included.\n",
    "# Library has DsRed, ZsYellow, GFP, Chl, ChlA, ChlB, Noise\n",
    "fluorophores=(GFP Chl Noise) # Order doesn't matter here. Names must match library.\n",
    "desired_wavelength_range=(500 900) # (first last), e.g. (500 900)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing false-color plots for fluorescent proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FalseColor_channels=(Chl GFP Noise) # (Red Green Blue), e.g. (Chl GFP Noise)\n",
    "FalseColor_caps=(200 400 200) # (Red Green Blue); recommend 400 for reporters, 200 for others; e.g. (200 400 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producing summary statistics for fluorescent proteins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporters=(GFP Chl) # Will compute summary stats for these proteins, e.g. (GFP) or (GFP Chl)\n",
    "pixel_threshold=3 # If this many pixels... (recommended: 3)\n",
    "reporter_threshold=38 # ...have this much signal (recommended: 38), then the tissue is \"Positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cropping and alignment settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings and files for cropping to grid spots and aligning RGB/hyperspectral layers should be prepared according to [these tutorials](https://github.com/naglemi/GMOnotebook/tree/master/1_Decide_parameters/2_Align_and_crop_parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=12 # 12 or 20\n",
    "pre_aligned_resized_grid_borders=(\"ENTER_PRE_ALIGNED_GRID_BORDERS\") # \"(left,top,right,bottom)\"\n",
    "aligned_grid_borders=\"ENTER_ALIGNED_GRID_BORDERS\" # \"top bottom right left\"\n",
    "mode=\"scikit\" # \"scikit\" (recommended) or \"opencv\"\n",
    "homography=\"ENTER_HOMOGRAPHY_NPY\" # file path\n",
    "hypercube_csv=\"ENTER_HYPERCUBE_TO_CSV\" # file path\n",
    "aligned_grid=\"ENTER_ALIGNED_GRID\" # file path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite=0 # 1 to make composite images with side-by-side RGB, segmentation outputs and blended images (slow), 0 to skip\n",
    "test_align_each_img=0 # 1 to make blended images of aligned RGB and hyperspectral layers for inspection, 0 to skip\n",
    "width=9 # GGplot box/violin plot output (inches)\n",
    "height=5 # GGplot box/violin plot output (inches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel=0 # 1 if parallelizing CubeGLM with GNU Parallel, 0 if not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths to workflow modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These only need to be modified if you are setting up a `GMOnotebook` template on a new computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd=\"/home/gmobot/GMOnotebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmodetector_wd=\"/home/gmobot/gmodetector_py/\"\n",
    "spectral_library_path=\"${gmodetector_wd}spectral_library/\"\n",
    "deeplab_path=\"/home/gmobot/poplar_model_2_w_contam/\"\n",
    "alignment_path=\"/home/gmobot/ImageAlignment/\"\n",
    "gmolabeler_path=\"/home/gmobot/GMOlabeler/\"\n",
    "contamination_path=\"/home/gmobot/DenseNet\"\n",
    "data_prefix=\"/media/gmobot/Easystore_longerm_storage_2/data/\"\n",
    "output_directory_prefix=\"${data_prefix}gmodetector_out/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "With all above variables set, please \"Save as...\" with a filename referencing this specific dataset. <br>Finally, deploy the workflow (Step 4 in above instructions).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automated workflow to be deployed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the below code for a walkthrough of how GMOnotebook works, or view the outputs after running the workflow for help troubleshooting errors in specific steps of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\"> <b>Danger:</b> Do not modify any below code without creating a new version of the template notebook. During routine usage, this workflow should be customized only by modifying variables above, while leaving the below code unmodified. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These internal variables are set automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datestamp=$(date +”%Y-%m-%d”)\n",
    "data_folder=$(echo $data | cut -d/ -f5-)\n",
    "timepoint=\"$(basename -- $data_folder)\"\n",
    "output_directory_full=\"$output_directory_prefix$data_folder\"\n",
    "dataset_name=$(echo $data_folder | sed -e 's/\\///g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time analysis begins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantification of fluorescent proteins by regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python package `CubeGLM` is used to quantify fluorescent proteins in each pixel of hyperspectral images via linear regression. Hyperspectral images are regressed over spectra of known components, and pixelwise maps of test-statistics are constructed for each component in the sample. This approach to quantifying components of hyperspectral images is described in-depth in the Methods section from <a href=\"https://link.springer.com/article/10.1007/s40789-019-0252-7\" target=\"_blank\">Böhme, et al. 2019</a>. Code and documentation for `CubeGLM` is on <a href=\"https://github.com/naglemi/GMOdetector_py\" target=\"_blank\">Github</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $gmodetector_wd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_list_name=\"$dataset_name.jobs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -rf $job_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in $data/*.hdr\n",
    "do\n",
    " if [[ \"$file\" != *'hroma'* ]] && [[ \"$file\" != *'roadband'* ]]; then\n",
    "  echo \"python -W ignore wrappers/analyze_sample.py \\\n",
    "--file_path $file \\\n",
    "--fluorophores ${fluorophores[*]} \\\n",
    "--min_desired_wavelength ${desired_wavelength_range[0]} \\\n",
    "--max_desired_wavelength ${desired_wavelength_range[1]} \\\n",
    "--red_channel ${FalseColor_channels[0]} \\\n",
    "--green_channel ${FalseColor_channels[1]} \\\n",
    "--blue_channel ${FalseColor_channels[2]} \\\n",
    "--red_cap ${FalseColor_caps[0]} \\\n",
    "--green_cap ${FalseColor_caps[1]} \\\n",
    "--blue_cap ${FalseColor_caps[2]} \\\n",
    "--plot 1 \\\n",
    "--spectral_library_path \"$spectral_library_path\" \\\n",
    "--output_dir $output_directory_full \\\n",
    "--threshold 38\" >> $job_list_name\n",
    " fi\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $job_list_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda activate cubeglm\n",
    "\n",
    "if [ $parallel -eq 1 ]\n",
    "then\n",
    "    parallel --jobs 20 -a $job_list_name\n",
    "fi\n",
    "\n",
    "if [ $parallel -eq 0 ]\n",
    "then\n",
    "    bash $job_list_name\n",
    "fi\n",
    "\n",
    "conda deactivate\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time regression completes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo $(date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks to segment tissues, classify missing/contaminated explants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic segmentation of tissues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images are segmented into specific plant tissues by a deep neural network of the state-of-the-art Deeplab v3 architecture <a href=\"https://arxiv.org/abs/1706.05587\" target=\"_blank\">Liang-Chieh et al., 2017</a>. The model has been trained using training sets generated with our annotation GUI Intelligent DEep Annotator for Segmentation (IDEAS, available on <a href=\"https://bitbucket.org/JialinYuan/image-annotator/src/master/\" target=\"_blank\">Bitbucket</a>, publication pending). Our branch of the Deeplab v3 repo, including a Jupyter walkthrough for training, can be found on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is completed upstream of this notebook, which only entails analysis of test data using the latest model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/downsized/segmentation_composite2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: This example image was taken from an experiment on the effects of different CIMs on cottonwood regeneration. This composite image illustrates that for every sample, tissues are segmented into stem (red), callus (blue) and shoot (green). These composite images, useful for manual inspection of results, are produced when the 'composite' option is on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Normalize orientation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We desire for images to all be in the same orientation. At one point, the camera on the *macroPhor Array* was set to automatically detect orientation, which led to images randomly being in portrait or landscape. Here we will standardize the orientation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate alignment\n",
    "for filename in $data/*.jpg; do\n",
    "    exiftool -Orientation=8 -n $filename > ${data}log_exiftool.txt\n",
    "    done\n",
    "conda deactivate\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -f $data/*original*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Crop and resize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script resizes images to 900x900 and then crops away top and bottom 150 pixels for a final image size of 900x600."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose for cropping is to remove labels, which has been standard practice for all training and testing. Otherwise, we could run into problems such as the neural network \"learning\" plants labeled as control have more or less regeneration.<br>The purpose for resizing is to reduce computational expense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ${cwd}/intermediates/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate base\n",
    "python crop.py $data\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prepare input list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script `inference.py` requires a list of all files to be analyzed. We will create this file as `test.csv`. This will be a list of all our (pre-processed) image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $data\n",
    "ls -d $PWD/* $data | grep -i \"rgb_cropped.jpg\" > test.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the chroma standard from list of RGB image data to be segmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sed -i '/hroma/d' \"${data}/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trained model is deployed to perform semantic segmentation of experimental images. A list of RGB images to be segmented by the trained model is passed through the --image-list option. For each of these images, we will obtain an output mask (.png) of labeled tissues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies include `opencv`, `scipy`, `yaml` and `tensorflow` (version 1.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cd $deeplab_path\n",
    "conda activate deeplab\n",
    "export PYTHONPATH=$PYTHONPATH:`pwd`:`pwd`/slim\n",
    "python -W ignore deeplab/inference.py \\\n",
    "--image_lists \"${data}/test.csv\" \\\n",
    "--crop_size 900 --crop_size 600 \\\n",
    "--seg_results segmentation_results \\\n",
    "--model_dir \"${deeplab_path}/deeplab/model/\"\n",
    ">> $data/log_inference.txt\n",
    "conda deactivate\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean up and reorganize files. Let's keep the outputs we need with all other data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mv \"${deeplab_path}/segmentation_results/raw/\"* $data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name outputs to reflect that they are segmentation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd $data\n",
    "for file in *_rgb_cropped.png; do mv -f \"$file\" \"${file%_rgb_cropped.png}_segment_cropped.png\"; done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-expand segment outputs to same size as original RGB files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate alignment\n",
    "cd $alignment_path\n",
    "python expand.py $data >> $data/log_expand.txt\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make composite images with side-by-side RGB, segmentation outputs and blended images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $composite -eq 1 ]\n",
    "then\n",
    "    conda activate test-environment\n",
    "    cd $gmolabeler_path\n",
    "    python image_blender.py $data 0.75 'both' 1 180 >> $data/log_blend.txt\n",
    "    conda deactivate\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification of contaminated/missing explants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plates are cropped into sub-images for each explant and each is analyzed to determine if the explant position should be excluded from analysis due to being missing or contamination. Missing and contaminated explants are recognized using a trained Densenet model (<a href=\"https://github.com/Contamination-Classification/DenseNet\" target=\"_blank\">Huang, et al. 2018</a>). Our fork of the Densenet repository is available on <a href=\"https://arxiv.org/abs/1608.06993\" target=\"_blank\">GitHub</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/Densenet.png\">\n",
    "Figure: These are four examples of contaminated explants used in the training set for this pre-trained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the grid cropping dimensions, we can run the following script. Note that these are the dimensions to crop the image to after resizing to 2000x2000 (from 4000x4000 in the case of the *macroPhor Array)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare list of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $missing_explants = \"Automatic\" ]; then\n",
    "    echo \"Missing explants will be inferred.\"\n",
    "    cd $data\n",
    "    ls -d $PWD/* $data | grep -i \"rgb.jpg\" > rgb_list.txt\n",
    "    sed -i '/hroma/d' rgb_list.txt\n",
    "    img_list_path=\"${data}/rgb_list.txt\"\n",
    "else\n",
    "    echo \"Missing explants input manually by user, in file: \"\n",
    "    echo $missing_explants\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the mode for missing explant data is automatic, prepare input file for script to detect missing explants and run this script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Infer contaminated/missing explants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "if [ $missing_explants = \"Automatic\" ]; then\n",
    "    cd $contamination_path\n",
    "    conda activate DenseNet\n",
    "    python -W ignore inference.py \\\n",
    "    --img-list=$img_list_path \\\n",
    "    --crop_dims $pre_aligned_resized_grid_borders \\\n",
    "    --output_file=output.csv >> $data/log_contam.txt\n",
    "    mv -f output.csv \"${data}/output.csv\"\n",
    "    conda deactivate\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $missing_explants = \"Automatic\" ]; then\n",
    "    missing_explants=\"${data}/output.csv\"\n",
    "    echo \"Missing explants inferred by model and written to file:\"\n",
    "    echo $missing_explants\n",
    "else\n",
    "    echo \"Missing explants input manually by user, in file: \"\n",
    "    echo $missing_explants\n",
    "fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alignment of RGB and hyperspectral layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To match the frame and angle of RGB and hyperspectral image layers, we perform a homography transformation using a method described [in these notebooks](https://github.com/naglemi/GMOnotebook/tree/master/1_Decide_parameters/2_Align_and_crop_parameters/2_find_alignment_parameters). Using a pair of standard images, a homography matrix is calculated for the necessary transformation of RGB images to align with hyperspectral images. The transformation can then be applied to large batches of images rapidly, as long as the RGB and hyperspectral cameras remain in the same positions relative to one another (as they do in the macroPhor Array platform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/Alignment.png\">\n",
    "Figure: To enable precise calculation of a homography matrix for transformation of RGB images to match hyperspectral images, we used images of a piece of paper with grid marks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `opencv` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare file lists for alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\"><b>Tip:</b> We will produce two lists: one for hyperspectral channels (chlorophyll peak channel) for each sample and another for the complementary RGB images. We will superimpose them and produce images for inspection, allowing the user to make sure the alignment works reliably for all images. However, it is possible to replace the hyperspectral channels for each image with a single file; this would run more quickly but be less useful for allowing the user to validate alignment results.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to produce a csv with two columns with headers `hyper_img` and `rgb_images`. For each RGB image being transformed in batch alignment (mode 2), we can test the alignment by producing superimposed images of the transformed RGB images together with a hyperspectral layer. The hyperspectral layer can either be for a grid (fast) or can be for a layer extracted from the hyperspectral image of each channel (slow, but useful for making sure a certain homography matrix works reliably to transform a batch of images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $mode = \"opencv\" ]; then # Generate list for column column `rgb_images`, then another for hyperspectral, and merge\n",
    "    cd $data\n",
    "    ls | grep -i 'rgb\\.jpg' > file_list_part1.csv\n",
    "    ls | grep -i 'segment_uncropped\\.png' > file_list_part2.csv\n",
    "    cat file_list_part* > file_list.csv\n",
    "    # sed -i '/hroma/d' file_list.csv\n",
    "    cwd2=$(pwd)/\n",
    "    awk -v prefix=\"$cwd2\" '{print prefix $0}' file_list.csv > temp\n",
    "    mv -f temp file_list.csv\n",
    "    echo 'rgb_images' | cat - file_list.csv > temp && mv -f temp file_list.csv\n",
    "    cp file_list.csv file_list_hyper_channel.csv\n",
    "    sed -i 's/_rgb.jpg/_hyperchannel.csv/g' file_list_hyper_channel.csv\n",
    "    sed -i 's/rgb_images/hyper_img/g' file_list_hyper_channel.csv\n",
    "    sed -i 's/_segment_uncropped.png/_hyperchannel.csv/g' file_list_hyper_channel.csv\n",
    "    paste --delimiters=',' file_list_hyper_channel.csv file_list.csv > rgb_and_hyper_channel_lists.csv\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run batch alignment to apply homography matrix to all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $mode = \"opencv\" ]; then\n",
    "    conda activate alignment\n",
    "    cd $alignment_path\n",
    "    file_list_input=\"${data}/rgb_and_hyper_channel_lists.csv\"\n",
    "    python main.py \\\n",
    "    --hyper-img $hypercube_csv \\\n",
    "    --img-csv $file_list_input \\\n",
    "    --mode 2 \\\n",
    "    --h_matrix_path $homography >> $data/log_alignment_mode2.txt\n",
    "    conda deactivate\n",
    "    conda deactivate\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `scikit` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this similar, but simpler approach, a homography matrix is manually generated by the user as shown in the `manual_alignment` notebooks in the GMOnotebook repository. This is notably different from how these matrices are automatically generated with the `opencv` method (which is not always perfectly robust). This homography matrix is then used to batch-transform all images for this dataset, similarly to the `opencv` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypercube_jpg=$(echo $hypercube_csv | sed -e 's/\\.csv/.jpg/g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if [ $mode = \"scikit\" ]; then\n",
    "    conda activate alignment\n",
    "    python $cwd/manual_alignment/batch_align_manual.py \\\n",
    "    --img_dir $data \\\n",
    "    --hyp_path $hypercube_jpg \\\n",
    "    --matrix $homography\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-analyze deep segmentation and regression results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scripts in the <a href=\"https://github.com/naglemi/GMOlabeler\" target=\"_blank\">GMOlabeler repository</a> are used to cross-reference results from deep segmentation of RGB images and regression of hyperspectral imaging, apply thresholding parameters to classify tissues as transgenic or escapes, and produce plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Figures/GMOlabeler.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure: The various steps of data processing in GMOlabeler are illustrated for an example explant from an experiment on CIM optimization for cottonwood. Images of plates are cropped to a sub-image for each explant. RGB segmentation results and hyperspectral regression results are cross-referenced to calculate fluorecent proteins in specific tissues and infer whether these tissues are transgenic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare sample datasheet input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare input file we will use for making plots. This file contains paths to CLS results, RGB images, and hyperspectral images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "conda activate gmolabeler\n",
    "cd \"${cwd}/intermediates/\"\n",
    "Rscript pre_label.R \\\n",
    "-r \"${data}/\" \\\n",
    "-R \"${output_directory_prefix}\" \\\n",
    "-i 1 \\\n",
    "-d $datestamp\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-reference RGB and hyperspectral data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate gmolabeler\n",
    "cd $gmolabeler_path\n",
    "for reporter in ${reporters[@]}; do\n",
    "    python main.py \\\n",
    "    \"${data}/samples_pre_labeling.csv\" \\\n",
    "    $aligned_grid \\\n",
    "    $reporter_threshold \\\n",
    "    $reporter \\\n",
    "    $grid \\\n",
    "    \"hdf\" \\\n",
    "    $gmolabeler_path \\\n",
    "    \"$aligned_grid_borders\" > $data/log_gmolabeler_$reporter.txt\n",
    "done\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate sums of statistics over combined segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in all regenerated tissue (callus + shoot) as well as all tissue (including stem as well). We will calculate aggregate statistics over these groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate gmolabeler\n",
    "for reporter in ${reporters[@]}; do\n",
    "    Rscript calculate_sum_stats_over_combined_segments.R \\\n",
    "    --output_dir \"${gmolabeler_path}/output/\" \\\n",
    "    --datapath \"${data_folder}/${reporter}/\"\n",
    "done\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make plots of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "conda activate gmolabeler\n",
    "cd $gmolabeler_path\n",
    "for reporter in ${reporters[@]}; do\n",
    "    Rscript grid_item_plots.R \\\n",
    "    -d \"${data_folder}/\" \\\n",
    "    -r \"$randomization_datasheet\" \\\n",
    "    -p $pixel_threshold \\\n",
    "    -v categorical \\\n",
    "    -m 1 \\\n",
    "    -M $missing_explants \\\n",
    "    -g $grid \\\n",
    "    --samples-pre-labeling ${data}/samples_pre_labeling.csv \\\n",
    "    --sort 1 \\\n",
    "    --height $height \\\n",
    "    --width $width \\\n",
    "    --Reporter $reporter\n",
    "done\n",
    "conda deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean-up some outputs, re-organize some files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rm -f ${output_directory_full}*_weights.hdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo \"${gmolabeler_path}/plots${data_folder}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"${gmolabeler_path}/plots/${data_folder}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -f ./plants_over_plates.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "echo ${gmolabeler_path}/output/${data_folder}/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for reporter in ${reporters[@]}; do\n",
    "    cp \"${gmolabeler_path}/output/${data_folder}/${reporter}/stats_with_sums_over_tissues.csv\" ./\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm -f Rplots.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
